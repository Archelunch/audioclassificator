{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "nUS_Jf5XWfeX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "from scipy.io import wavfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Kv_oE0_2hPaD"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import sys\n",
    "import glob\n",
    "from json import dumps, loads\n",
    "import os\n",
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform, color\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import cv2\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "wMaoVnnLknw7"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "xxJq-IK8gqCB"
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=8):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(0.3))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(p=0.3))\n",
    "        self.fc = nn.Linear(12*12*64, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "L1vO1nj7FSHt"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "HdlYwLnmG0CB"
   },
   "outputs": [],
   "source": [
    "plot.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "ImuzdR_-iIrj"
   },
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    \"\"\"Face dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_path = pd.read_csv(csv_file, sep=\"\\t\", names=['name', 'place', 'zero', 'score', 'class'], header=None)\n",
    "        self.num_classes = len(self.image_path['class'].unique())\n",
    "        self.class_dict = {}\n",
    "        for idx in range(len(self.image_path['class'].unique())):\n",
    "          self.class_dict[self.image_path['class'].unique()[idx]] = idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_path)\n",
    "    \n",
    "    \n",
    "    def get_specgram(self, path):\n",
    "      samplingFrequency, signalData = wavfile.read(path)\n",
    "      \n",
    "      fig = plot.figure()\n",
    "      fig.add_subplot(111)\n",
    "      plot.grid(False)\n",
    "      plot.axis('off')\n",
    "      plot.specgram(signalData,Fs=samplingFrequency)\n",
    "      fig.canvas.draw()\n",
    "      plot.close()\n",
    "      return np.array(fig.canvas.renderer._renderer)\n",
    "    \n",
    "    \n",
    "    def random(self):\n",
    "      self.image_path = shuffle(self.image_path)\n",
    "    \n",
    "    \n",
    "    def batches(self, batch_size):\n",
    "      for i in range(0, len(self.image_path), batch_size):\n",
    "        path_t = list(self.image_path[i:i+batch_size]['name'])\n",
    "        pearson_t = list(self.image_path[i:i+batch_size]['class'])\n",
    "        batch_img = torch.ones(len(path_t), 4, 48, 48)\n",
    "        batch_pearson = torch.ones(len(path_t))\n",
    "        for j in range(len(path_t)):\n",
    "          image = self.get_specgram(os.path.join(self.root_dir, path_t[j]))\n",
    "          pearson = self.class_dict[pearson_t[j]]\n",
    "          sample = {'image': image, 'pearson': pearson}\n",
    "          if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "          batch_img[j] = sample['image']\n",
    "          batch_pearson[j] = sample['pearson']\n",
    "        yield batch_img, batch_pearson\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_path.loc[idx, 'name']\n",
    "        image = self.get_specgram(os.path.join(self.root_dir, img_name))\n",
    "        pearson = self.class_dict[self.image_path.loc[idx, 'class']]\n",
    "        sample = {'image': image, 'pearson': pearson}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample['image'], sample['pearson']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "SEl1f0Kfi36e"
   },
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, pearson = sample['image'], sample['pearson']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "        \n",
    "        new_size = max(int(new_h), int(new_w))\n",
    "        new_h, new_w = new_size, new_size\n",
    "\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "\n",
    "        return {'image': img, 'pearson': pearson}\n",
    "    \n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, pearson = sample['image'], sample['pearson']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'pearson': torch.LongTensor([pearson])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "XZypntxqkMRE"
   },
   "outputs": [],
   "source": [
    "audio_dataset = AudioDataset(csv_file='meta/meta.txt', root_dir='audio', transform=transforms.Compose([\n",
    "                                               Rescale(32),\n",
    "                                               ToTensor()\n",
    "                                           ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "_vLCwZ_kkqs1"
   },
   "outputs": [],
   "source": [
    "audio_dataset.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2210,
     "status": "ok",
     "timestamp": 1530721452960,
     "user": {
      "displayName": "Михаил Павлухин",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "113034321969470741624"
     },
     "user_tz": -180
    },
    "id": "1lr2Xr0Al_bU",
    "outputId": "6777de58-1e4f-4c1d-8347-b38b5eaaf02e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "LRlDUCpcmCu2"
   },
   "outputs": [],
   "source": [
    "model = ConvNet(8).to(device)\n",
    "#model.load_state_dict(torch.load(\"mytraining.pt\")) #Можно не тренировать модель, а сразу загрузить веса\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-BMZWNCHmLyF",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e45058427d4027abfa1d20d01a2941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/5], Loss: 0.4492\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a131a97cd344b3b2839e9c217b8713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [2/5], Loss: 0.3106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "During request exception was raised: <urlopen error [WinError 10055] Невозможно выполнить операцию на сокете, т.к. буфер слишком мал или очередь переполнена>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\", line 1318, in do_open\n",
      "    encode_chunked=req.has_header('Transfer-encoding'))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\http\\client.py\", line 1239, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\http\\client.py\", line 1285, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\http\\client.py\", line 1234, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\http\\client.py\", line 1026, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\http\\client.py\", line 964, in send\n",
      "    self.connect()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\http\\client.py\", line 1392, in connect\n",
      "    super().connect()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\http\\client.py\", line 936, in connect\n",
      "    (self.host,self.port), self.timeout, self.source_address)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\socket.py\", line 724, in create_connection\n",
      "    raise err\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\socket.py\", line 713, in create_connection\n",
      "    sock.connect(sa)\n",
      "OSError: [WinError 10055] Невозможно выполнить операцию на сокете, т.к. буфер слишком мал или очередь переполнена\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\telepyth\\client.py\", line 64, in __call__\n",
      "    res = urlopen(req)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\", line 223, in urlopen\n",
      "    return opener.open(url, data, timeout)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\", line 526, in open\n",
      "    response = self._open(req, data)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\", line 544, in _open\n",
      "    '_open', req)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\", line 504, in _call_chain\n",
      "    result = func(*args)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\", line 1361, in https_open\n",
      "    context=self._context, check_hostname=self._check_hostname)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\", line 1320, in do_open\n",
      "    raise URLError(err)\n",
      "urllib.error.URLError: <urlopen error [WinError 10055] Невозможно выполнить операцию на сокете, т.к. буфер слишком мал или очередь переполнена>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c0ab835999455ebafaec4f9abeea0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [3/5], Loss: 0.1878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "During request exception was raised: <urlopen error [WinError 10055] Невозможно выполнить операцию на сокете, т.к. буфер слишком мал или очередь переполнена>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\", line 1318, in do_open\n",
      "    encode_chunked=req.has_header('Transfer-encoding'))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\http\\client.py\", line 1239, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\http\\client.py\", line 1285, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\http\\client.py\", line 1234, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\http\\client.py\", line 1026, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\http\\client.py\", line 964, in send\n",
      "    self.connect()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\http\\client.py\", line 1392, in connect\n",
      "    super().connect()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\http\\client.py\", line 936, in connect\n",
      "    (self.host,self.port), self.timeout, self.source_address)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\socket.py\", line 724, in create_connection\n",
      "    raise err\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\socket.py\", line 713, in create_connection\n",
      "    sock.connect(sa)\n",
      "OSError: [WinError 10055] Невозможно выполнить операцию на сокете, т.к. буфер слишком мал или очередь переполнена\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\telepyth\\client.py\", line 64, in __call__\n",
      "    res = urlopen(req)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\", line 223, in urlopen\n",
      "    return opener.open(url, data, timeout)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\", line 526, in open\n",
      "    response = self._open(req, data)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\", line 544, in _open\n",
      "    '_open', req)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\", line 504, in _call_chain\n",
      "    result = func(*args)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\", line 1361, in https_open\n",
      "    context=self._context, check_hostname=self._check_hostname)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\", line 1320, in do_open\n",
      "    raise URLError(err)\n",
      "urllib.error.URLError: <urlopen error [WinError 10055] Невозможно выполнить операцию на сокете, т.к. буфер слишком мал или очередь переполнена>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd2c5b7ceb042428200e44534ab244a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [4/5], Loss: 0.0925\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af217ab42e954b4aaaa21417c583eb28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [5/5], Loss: 0.1111\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "235315df1a184b92ab8536af4ff30b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [6/5], Loss: 0.0845\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bede2d1c17d74ca8a112b018d7af735e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [7/5], Loss: 0.0707\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dfb69f8f54742b28f817933812b280f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8/5], Loss: 0.0897\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542b475d396b4f3e9b6dbe5a3ae4ecf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [9/5], Loss: 0.0212\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb80b4b2ad64fe28ef3d468eb06f2d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [10/5], Loss: 0.0409\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "440ebe4ddf534dec9c5c7432b3d1bcc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [11/5], Loss: 0.0085\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79111b8831dc4ce5908a527a1f2a7623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [12/5], Loss: 0.0549\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ecf0be64eac4d37a7d5758a50485a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [13/5], Loss: 0.0064\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46ec993790084888aae93dce82979187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [14/5], Loss: 0.0332\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d6d68e37b3437c8a07027d7db1194d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [15/5], Loss: 0.0481\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca08290f61e40d4b519022dba49f1e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [16/5], Loss: 0.0166\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a926b752fc784360aed308c0619756a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [17/5], Loss: 0.0313\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d981cec84149a1a316e2e9b94d5a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(20):\n",
    "    for images, labels in tqdm(audio_dataset.batches(64)):\n",
    "        images = images.to(device)\n",
    "        labels = labels.long().to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}' \n",
    "           .format(epoch+1, 20, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "at = os.listdir(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_ser = pd.Series(at[:473])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c3ce119d06144fc8db5ebd547e89eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy of the model on the 473 val audio: 84.14376321353066 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in tqdm(range(0, len(t_ser), 64)):\n",
    "        path_t = list(t_ser[i:i+64])\n",
    "        batch_img = torch.ones(len(path_t), 4, 48, 48)\n",
    "        batch_pearson = torch.ones(len(path_t))\n",
    "        for j in range(len(path_t)):\n",
    "          image = audio_dataset.get_specgram(os.path.join(\"test\", path_t[j]))\n",
    "          nclass = path_t[j].split(\"_\")[0] if path_t[j].split(\"_\")[0] != \"knocking\" else \"_\".join(path_t[j].split(\"_\")[:2])\n",
    "          pearson = audio_dataset.class_dict[nclass]\n",
    "          sample = {'image': image, 'pearson': pearson}\n",
    "          if audio_dataset.transform:\n",
    "            sample = audio_dataset.transform(sample)\n",
    "          batch_img[j] = sample['image']\n",
    "          batch_pearson[j] = sample['pearson']\n",
    "        outputs = model(batch_img.to(device))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += batch_pearson.size(0)\n",
    "        correct += (predicted == batch_pearson.to(device).long()).sum().item()\n",
    "    print('Test Accuracy of the model on the {} val audio: {} %'.format(len(t_ser), 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d16fbbd667884c3194d57adc00b95ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "t_ser = pd.Series(at)\n",
    "res_path = []\n",
    "res_score = []\n",
    "res_class.. to load your previously training model: = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(t_ser), 64)):\n",
    "        path_t = list(t_ser[i:i+64])\n",
    "        batch_img = torch.ones(len(path_t), 4, 48, 48)\n",
    "        for j in range(len(path_t)):\n",
    "          image = audio_dataset.get_specgram(os.path.join(\"test\", path_t[j]))\n",
    "          sample = {'image': image, 'pearson': 404}\n",
    "          if audio_dataset.transform:\n",
    "            sample = audio_dataset.transform(sample)\n",
    "          batch_img[j] = sample['image']\n",
    "        outputs = model(batch_img.to(device))\n",
    "        scores, predicted = torch.max(outputs.data, 1)\n",
    "        res_path+=path_t\n",
    "        res_score+= list(scores.cpu().numpy())\n",
    "        res_class+= list(predicted.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_data = pd.DataFrame()\n",
    "ans_data['name'] = res_path\n",
    "ans_data['score'] = res_score\n",
    "ans_data['class'] = res_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ans_data['class'] = ans_data['class'].map({v:k for k,v in audio_dataset.class_dict.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Сохранил модель\n",
    "torch.save(model.state_dict(), \"mytraining.pt\")\n",
    "\n",
    "# Загрузка модели\n",
    "#model.load_state_dict(torch.load('mytraining.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ans_data.to_csv(\"result.txt\", sep=\"\\t\", index=False, header=None)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "AudioClassification.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
